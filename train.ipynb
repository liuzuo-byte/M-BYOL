{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from arguments import get_args\n",
    "from augmentations import get_aug\n",
    "from models import get_model\n",
    "from tools import AverageMeter, knn_monitor, Logger, file_exist_check\n",
    "from datasets import get_dataset\n",
    "from optimizers import get_optimizer, LR_Scheduler\n",
    "from linear_eval import main as linear_eval\n",
    "#from datetime import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import collections\n",
    "from torch import optim\n",
    "import torch\n",
    "from sklearn import metrics, preprocessing\n",
    "import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sys\n",
    "sys.path.append('./global_module/')\n",
    "import network\n",
    "import train\n",
    "from generate_pic import aa_and_each_accuracy, sampling,load_dataset, generate_png1, generate_iter\n",
    "from Utils import record, extract_samll_cubic\n",
    "import byol_pytorch\n",
    "import vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "def generate_iter_ViT(TRAIN_SIZE, train_indices, TEST_SIZE, test_indices, TOTAL_SIZE, total_indices, VAL_SIZE,\n",
    "                  whole_data, PATCH_LENGTH, padded_data, INPUT_DIMENSION, batch_size, gt,args=None):\n",
    "\n",
    "    gt_all = gt[total_indices] - 1\n",
    "\n",
    "    all_data = extract_samll_cubic.select_small_cubic(TOTAL_SIZE, total_indices, whole_data,\n",
    "                                                      PATCH_LENGTH, padded_data, INPUT_DIMENSION)\n",
    "\n",
    "    all_data.reshape(all_data.shape[0], all_data.shape[1], all_data.shape[2], INPUT_DIMENSION)\n",
    "    all_tensor_data = torch.from_numpy(all_data).type(torch.FloatTensor).permute(0,3,1,2)\n",
    "    all_tensor_data_label = torch.from_numpy(gt_all).type(torch.FloatTensor)\n",
    "    #print(all_tensor_data.shape)\n",
    "    #print(all_tensor_data_label.shape)\n",
    "    torch_dataset_all = Data.TensorDataset(all_tensor_data, all_tensor_data_label)\n",
    "\n",
    "    trainall_iter = Data.DataLoader(\n",
    "            dataset=torch_dataset_all,  # torch TensorDataset format\n",
    "            batch_size=batch_size,  # mini batch size\n",
    "            shuffle=True,  # 要不要打乱数据 (打乱比较好)\n",
    "            num_workers=0,  # 多线程来读数据\n",
    "        )\n",
    "    all_iter = Data.DataLoader(\n",
    "        dataset=torch_dataset_all,  # torch TensorDataset format\n",
    "        batch_size=batch_size,  # mini batch size\n",
    "        shuffle=False,  # 要不要打乱数据 (打乱比较好)\n",
    "        num_workers=0,  # 多线程来读数据\n",
    "    )\n",
    "    return all_iter,trainall_iter #, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#数据增强\n",
    "\n",
    "def applyPCA(X, numComponents=15):\n",
    "        newX = np.reshape(X, (-1, X.shape[2]))\n",
    "        pca = PCA(n_components=numComponents, whiten=True,svd_solver='full')#不加full会报数组错误\n",
    "        newX = pca.fit_transform(newX)\n",
    "        newX = np.reshape(newX, (X.shape[0], X.shape[1], numComponents))\n",
    "        return newX\n",
    "\n",
    "def stochastic_augment(x):\n",
    "    num = random.randint(0,7)\n",
    "    if num >= 0 and num < 4:\n",
    "        #print(x.shape)\n",
    "        aug = np.fliplr(x)\n",
    "        aug=aug.transpose(1,2,0)\n",
    "        np.random.seed(int(time.time()))#假如不设置随机数！！！！！！！！！！\n",
    "        #aug = cv2.GaussianBlur(np.float32(aug), (7, 7), wz)\n",
    "        noise=np.random.randint(0,2,(aug.shape[0],aug.shape[1]))\n",
    "        noise[aug.shape[0]//2,aug.shape[1]//2]=1#确保中心没有噪声\n",
    "        noise=noise[:,:,np.newaxis]\n",
    "        noise=np.concatenate([noise]*aug.shape[2],2)\n",
    "        aug=aug*noise\n",
    "        aug=aug.transpose(2,0,1)\n",
    "        Aug = torch.from_numpy(aug.copy())\n",
    "\n",
    "    elif num > 3 and num < 8:\n",
    "        #print(x.shape)\n",
    "        aug = np.flipud(x)\n",
    "        aug=aug.transpose(1,2,0)\n",
    "        np.random.seed(int(time.time()))#假如不设置随机数！！！！！！！！！！！！！\n",
    "        #aug = cv2.GaussianBlur(np.float32(aug), (7, 7), wz)\n",
    "        noise=np.random.randint(0,2,(aug.shape[0],aug.shape[1]))\n",
    "        noise[aug.shape[0]//2,aug.shape[1]//2]=1#确保中心没有噪声\n",
    "        noise=noise[:,:,np.newaxis]\n",
    "        noise=np.concatenate([noise]*aug.shape[2],2)\n",
    "        aug=aug*noise\n",
    "        aug=aug.transpose(2,0,1)\n",
    "        Aug = torch.from_numpy(aug.copy())\n",
    "    else:\n",
    "        Aug = x\n",
    "    return Aug\n",
    "\n",
    "def stochastic_augment_v2(x):\n",
    "    num = random.randint(0,7)\n",
    "    if num >= 0 and num < 4:\n",
    "        #print(x.shape)\n",
    "        aug = np.fliplr(x)\n",
    "        aug=aug.transpose(1,2,0)\n",
    "        np.random.seed(int(time.time()))#加入不设置随机数！！！！！！！！！！！！！！！\n",
    "        #aug = cv2.GaussianBlur(np.float32(aug), (7, 7), wz)\n",
    "        noise=np.ones((aug.shape[0],aug.shape[1]))\n",
    "        ran1=np.random.randint(0,aug.shape[0]-1)\n",
    "        ran2=np.random.randint(ran1+1,aug.shape[0])\n",
    "        ran3=np.random.randint(0,aug.shape[0]-1)\n",
    "        ran4=np.random.randint(ran3+1,aug.shape[0])\n",
    "        noise[ran1:ran2,ran3:ran4]=0\n",
    "        noise[aug.shape[0]//2,aug.shape[1]//2]=1#确保中心没有噪声\n",
    "        noise=noise[:,:,np.newaxis]\n",
    "        noise=np.concatenate([noise]*aug.shape[2],2)\n",
    "        aug=aug*noise\n",
    "        aug=aug.transpose(2,0,1)\n",
    "        Aug = torch.from_numpy(aug.copy())\n",
    "    elif num >3 and num < 8:\n",
    "        #print(x.shape)\n",
    "        aug = np.flipud(x)\n",
    "        aug=aug.transpose(1,2,0)\n",
    "        np.random.seed(int(time.time()))#假如不设置随机数！！！！！！！！！！！！！！！！\n",
    "        #aug = cv2.GaussianBlur(np.float32(aug), (7, 7), wz)\n",
    "        noise=np.ones((aug.shape[0],aug.shape[1]))\n",
    "        ran1=np.random.randint(0,aug.shape[0]-1)\n",
    "        ran2=np.random.randint(ran1+1,aug.shape[0])\n",
    "        ran3=np.random.randint(0,aug.shape[0]-1)\n",
    "        ran4=np.random.randint(ran3+1,aug.shape[0])\n",
    "        noise[ran1:ran2,ran3:ran4]=0\n",
    "        noise[aug.shape[0]//2,aug.shape[1]//2]=1#确保中心没有噪声\n",
    "        noise=noise[:,:,np.newaxis]\n",
    "        noise=np.concatenate([noise]*aug.shape[2],2)\n",
    "        aug=aug*noise\n",
    "        aug=aug.transpose(2,0,1)\n",
    "        Aug = torch.from_numpy(aug.copy())\n",
    "    return Aug\n",
    "\n",
    "def argument3(data):#，3D空间随机掩码,ratio为掩码率\n",
    "        np.random.seed(int(time.time()))\n",
    "        a = torch.rand_like(data)\n",
    "        zero = torch.zeros_like(data)\n",
    "        one = torch.ones_like(data)\n",
    "        b = torch.where(a > 0.1, one, zero) \n",
    "        return data*b\n",
    "\n",
    "def argument4(data):#，3D空间随机掩码,ratio为掩码率，中心像素不掩码\n",
    "        np.random.seed(int(time.time()))\n",
    "        a = torch.rand_like(data)\n",
    "        zero = torch.zeros_like(data)\n",
    "        one = torch.ones_like(data)\n",
    "        b = torch.where(a > 0.10, one, zero)\n",
    "        b[:,b.shape[1]//2,b.shape[2]//2] = 1 \n",
    "        return data*b\n",
    "\n",
    "def argument5(data):#，3D空间随机掩码,ratio为掩码率，光广播中心像素不掩码,range为广播范围!!!!\n",
    "        range=3\n",
    "        ratio=0.1\n",
    "        #中心像素先广播\n",
    "        centre = data[:,data.shape[1]//2,data.shape[1]//2]#中心像素[band]\n",
    "        centre2=centre.view(data.shape[0],1,1)#改变维度[band,1,1]\n",
    "        centre3=centre2.repeat(1,range,range)#centre变为[band,rang,rang]\n",
    "        data[:,data.shape[1]//2-range//2:data.shape[1]//2+range//2+1,data.shape[1]//2-range//2:data.shape[1]//2+range//2+1]=centre3#中心像素广播成功！！！！！！！\n",
    "        \n",
    "        #3D掩码，广播像素不掩码\n",
    "        a = torch.rand_like(data)\n",
    "        zero = torch.zeros_like(data)\n",
    "        one = torch.ones_like(data)\n",
    "        b = torch.where(a > ratio, one, zero)\n",
    "        b[:,b.shape[1]//2-range//2:b.shape[1]//2+range//2+1,b.shape[1]//2-range//2:b.shape[1]//2+range//2+1] = 1 \n",
    "        return data*b\n",
    "\n",
    "def radiation_noise(data, alpha_range=(0.6, 1.4), beta=1/5):\n",
    "        alpha = np.random.uniform(*alpha_range)\n",
    "        noise = np.random.normal(loc=0., scale=1.0, size=data.shape)\n",
    "        return alpha * data + beta * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(Dataset='sv', batch_size=256, device=device(type='cuda', index=0), epochs=20, lr=0.001, name='byolup', val_split=0.1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--device\",default=torch.device('cuda:0'),help=\"The model device\")\n",
    "parser.add_argument('--Dataset', default='sv',help='up,sv ,in or ksc')\n",
    "parser.add_argument('--lr', default= 1e-3 , help='the learning rate')\n",
    "parser.add_argument('--epochs', default=20)\n",
    "parser.add_argument('--batch_size', default=256)\n",
    "parser.add_argument('--name',default='byolup',help='保存的模型名')\n",
    "parser.add_argument('--val_split',default=0.1,help='测试集占多少')#不用这个了\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Importing Dataset-----\n",
      "###########data_hsi.shape###############\n",
      "(512, 217, 204)\n",
      "###########PCA_data_hsi.shape###############\n",
      "(512, 217, 15)\n",
      "$$$$$$$$$$$data.shape$$$$$$$$$$$$$$\n",
      "(111104, 15)\n",
      "The class numbers of the HSI data is: 16\n",
      "-----Importing Setting Parameters-----\n",
      "iter: 0\n",
      "Train size:  48714\n",
      "Test size:  5415\n",
      "Validation size:  100\n",
      "-----Selecting Small Pieces from the Original Cube Data-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/20: 100%|██████████| 212/212 [00:57<00:00,  3.69it/s]\n",
      "Training:   5%|▌         | 1/20 [00:57<18:13, 57.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最小loss，保存模型 19.595827873796225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 212/212 [00:57<00:00,  3.69it/s]\n",
      "Training:  10%|█         | 2/20 [01:54<17:15, 57.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最小loss，保存模型 1.0258496506139636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 212/212 [00:57<00:00,  3.67it/s]\n",
      "Training:  15%|█▌        | 3/20 [02:52<16:18, 57.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最小loss，保存模型 0.8157685147598386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 212/212 [00:57<00:00,  3.68it/s]\n",
      "Epoch 4/20: 100%|██████████| 212/212 [00:57<00:00,  3.67it/s]\n",
      "Epoch 5/20: 100%|██████████| 212/212 [00:58<00:00,  3.65it/s]\n",
      "Epoch 6/20: 100%|██████████| 212/212 [00:58<00:00,  3.64it/s]\n",
      "Epoch 7/20: 100%|██████████| 212/212 [00:57<00:00,  3.66it/s]\n",
      "Epoch 8/20: 100%|██████████| 212/212 [00:58<00:00,  3.65it/s]\n",
      "Epoch 9/20: 100%|██████████| 212/212 [00:58<00:00,  3.65it/s]\n",
      "Epoch 10/20: 100%|██████████| 212/212 [00:57<00:00,  3.67it/s]\n",
      "Epoch 11/20: 100%|██████████| 212/212 [00:57<00:00,  3.66it/s]\n",
      "Epoch 12/20: 100%|██████████| 212/212 [00:57<00:00,  3.67it/s]\n",
      "Epoch 13/20: 100%|██████████| 212/212 [00:58<00:00,  3.65it/s]\n",
      "Epoch 14/20: 100%|██████████| 212/212 [00:58<00:00,  3.64it/s]\n",
      "Epoch 15/20: 100%|██████████| 212/212 [00:58<00:00,  3.65it/s]\n",
      "Epoch 16/20: 100%|██████████| 212/212 [00:58<00:00,  3.65it/s]\n",
      "Epoch 17/20: 100%|██████████| 212/212 [00:57<00:00,  3.66it/s]\n",
      "Epoch 18/20: 100%|██████████| 212/212 [00:58<00:00,  3.65it/s]\n",
      "Epoch 19/20: 100%|██████████| 212/212 [00:58<00:00,  3.65it/s]\n",
      "Training: 100%|██████████| 20/20 [19:18<00:00, 57.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练结束 2.4429661091417074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = args.device\n",
    "seeds = [1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341]\n",
    "day = datetime.datetime.now()\n",
    "day_str = day.strftime('%m_%d_%H_%M')\n",
    "\n",
    "print('-----Importing Dataset-----')\n",
    "global Dataset  # UP,IN,KSC\n",
    "dataset = args.Dataset\n",
    "#dataset='up'\n",
    "Dataset = dataset.upper()\n",
    "data_hsi, gt_hsi, TOTAL_SIZE, TRAIN_SIZE, VALIDATION_SPLIT = load_dataset(Dataset)\n",
    "print(\"###########data_hsi.shape###############\")\n",
    "print(data_hsi.shape)\n",
    "data_hsi =applyPCA(data_hsi,15)\n",
    "print(\"###########PCA_data_hsi.shape###############\")\n",
    "print(data_hsi.shape)\n",
    "image_x, image_y, BAND = data_hsi.shape\n",
    "data = data_hsi.reshape(np.prod(data_hsi.shape[:2]), np.prod(data_hsi.shape[2:]))\n",
    "print(\"$$$$$$$$$$$data.shape$$$$$$$$$$$$$$\")\n",
    "print(data.shape)\n",
    "gt = gt_hsi.reshape(np.prod(gt_hsi.shape[:2]),)\n",
    "CLASSES_NUM = max(gt)\n",
    "print('The class numbers of the HSI data is:', CLASSES_NUM)\n",
    "\n",
    "print('-----Importing Setting Parameters-----')\n",
    "ITER = 1\n",
    "PATCH_LENGTH =13\n",
    "# number of training samples per class\n",
    "#lr, num_epochs, batch_size = 0.0010, 200, 32\n",
    "lr, num_epochs, batch_size = args.lr, args.epochs, args.batch_size\n",
    "\n",
    "# loss = torch.nn.CrossEntropyLoss()#将这里注释，看看有什么问题\n",
    "\n",
    "img_rows = 2*PATCH_LENGTH+1\n",
    "img_cols = 2*PATCH_LENGTH+1\n",
    "img_channels = data_hsi.shape[2]\n",
    "INPUT_DIMENSION = data_hsi.shape[2]\n",
    "ALL_SIZE = data_hsi.shape[0] * data_hsi.shape[1]\n",
    "VAL_SIZE = int(TRAIN_SIZE)\n",
    "TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE\n",
    "\n",
    "KAPPA = []\n",
    "OA = []\n",
    "AA = []\n",
    "TRAINING_TIME = []\n",
    "TESTING_TIME = []\n",
    "ELEMENT_ACC = np.zeros((ITER, CLASSES_NUM))\n",
    "\n",
    "data = preprocessing.scale(data)\n",
    "data_ = data.reshape(data_hsi.shape[0], data_hsi.shape[1], data_hsi.shape[2])\n",
    "whole_data = data_\n",
    "padded_data = np.lib.pad(whole_data, ((PATCH_LENGTH, PATCH_LENGTH), (PATCH_LENGTH, PATCH_LENGTH), (0, 0)),\n",
    "                         'constant', constant_values=0)\n",
    "\n",
    "for index_iter in range(ITER):\n",
    "    print('iter:', index_iter)\n",
    "    #net = network.FDSSC_network(BAND, CLASSES_NUM)\n",
    "        # define model\n",
    "    #model=vit.ViT(image_size = PATCH_LENGTH*2+1,patch_size = 5,num_classes = CLASSES_NUM,dim = 1024,depth = 4,heads = 16,mlp_dim = 2048,\n",
    "    model=vit.ViT(image_size = PATCH_LENGTH*2+1,patch_size = 3,dim = 256,depth = 2,heads = 16,mlp_dim =512,\n",
    "                  pool = 'cls', channels = BAND, dim_head = 32,dropout = 0.,emb_dropout = 0.,device=device) \n",
    "    #model=torch.load('./results/byol_0122212043.pt')\n",
    "    #model.load_state_dict(torch.load('./results/byol_0122203240.pt', map_location=device))\n",
    "    model=model.to(device)\n",
    "    '''\n",
    "    learner=byol_pytorch.BYOL(model,image_size=PATCH_LENGTH*2+1,channels=BAND,hidden_layer = -2,projection_size = 512,\n",
    "        projection_hidden_size = 1024,\n",
    "        augment_fn = stochastic_augment,\n",
    "        augment_fn2 = stochastic_augment_v2,\n",
    "        moving_average_decay = 0.99,\n",
    "        use_momentum = True)\n",
    "    '''\n",
    "    learner=byol_pytorch.BYOL(model,image_size=PATCH_LENGTH*2+1,channels=BAND,hidden_layer = -1,projection_size =64,\n",
    "        projection_hidden_size =1024,\n",
    "        # augment_fn = stochastic_augment,\n",
    "        # augment_fn2 = stochastic_augment_v2,\n",
    "        augment_fn = argument4,\n",
    "        augment_fn2 = argument4,\n",
    "        moving_average_decay = 0.99,\n",
    "        use_momentum = True)\n",
    "    optimizer = optim.Adam(learner.parameters(), lr=lr)  # , weight_decay=0.0001)\n",
    "    # optimizer = optim.SGD(learner.parameters(), lr=lr)  # , weight_decay=0.0001)\n",
    "    time_1 = int(time.time())\n",
    "    np.random.seed(seeds[index_iter])\n",
    "    train_indices, test_indices = sampling(args.val_split, gt)\n",
    "    _, total_indices = sampling(1, gt)\n",
    "\n",
    "    TRAIN_SIZE = len(train_indices)\n",
    "    print('Train size: ', TRAIN_SIZE)\n",
    "    TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE\n",
    "    print('Test size: ', TEST_SIZE)\n",
    "    VAL_SIZE = 100#实际上没用，我没用这个\n",
    "    print('Validation size: ', VAL_SIZE)\n",
    "\n",
    "    print('-----Selecting Small Pieces from the Original Cube Data-----')\n",
    "\n",
    "    all_iter,trainall_iter = generate_iter_ViT(TRAIN_SIZE, train_indices, TEST_SIZE, test_indices, TOTAL_SIZE, total_indices, VAL_SIZE,\n",
    "                  whole_data, PATCH_LENGTH, padded_data, INPUT_DIMENSION, batch_size, gt)\n",
    "    # Start training\n",
    "    global_progress = tqdm(range(0, num_epochs), desc=f'Training')\n",
    "    loss_record=[]\n",
    "    for epoch in global_progress:\n",
    "        model.train()\n",
    "        \n",
    "        local_progress=tqdm(trainall_iter, desc=f'Epoch {epoch}/{num_epochs}')#用所有的进行训练\n",
    "        loss_epoch=0\n",
    "        for idx, (images, labels) in enumerate(local_progress):\n",
    "            #print(images.shape)\n",
    "            loss = learner(images)\n",
    "            optimizer.zero_grad()\n",
    "            loss_epoch=loss_epoch+loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            learner.update_moving_average() # update moving average of target encoder\n",
    "        loss_record.append(loss_epoch)\n",
    "        if loss_epoch<=min(loss_record):\n",
    "            print('最小loss，保存模型',loss_epoch)\n",
    "            # model_path = os.path.join('./results/', f\"{args.name}_{datetime.datetime.now().strftime('%m%d%H%M%S')}_epoch{epoch}.pt\") # datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            model_path = os.path.join('./results/byolsv_a44_mask0.1.pt')#！！！！！！！！！保存模型！！！！！！！！！！！！！！！！\n",
    "            torch.save(model.state_dict(),model_path)\n",
    "\n",
    "        #if args.train.knn_monitor and epoch % args.train.knn_interval == 0: \n",
    "        #if True and epoch % 1 ==0:\n",
    "            #accuracy = knn_monitor(model.module.backbone, memory_loader, test_loader, device, k=min(200, len(memory_loader.dataset)), hide_progress=args.hide_progress) \n",
    "        \n",
    "        #epoch_dict = {\"epoch\":epoch, \"accuracy\":accuracy}\n",
    "        #global_progress.set_postfix(epoch_dict)\n",
    "        #logger.update_scalers(epoch_dict)\n",
    "    print('训练结束',loss_epoch)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg\n",
      "/root/anaconda3/envs/lzz/lib/python3.8/site-packages/matplotlib/mpl-data/matplotlibrc\n",
      "['GTK3Agg', 'GTK3Cairo', 'MacOSX', 'nbAgg', 'Qt4Agg', 'Qt4Cairo', 'Qt5Agg', 'Qt5Cairo', 'TkAgg', 'TkCairo', 'WebAgg', 'WX', 'WXAgg', 'WXCairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.matplotlib_fname()\n",
    "print(matplotlib.get_backend())\n",
    "\n",
    "print(matplotlib.matplotlib_fname())\n",
    "import matplotlib.rcsetup as rcsetup\n",
    "print(rcsetup.all_backends)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('lz')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf24a06100668a60a0ee6849ee174139413d0efcd8f32fc7ecf3671aafcd1762"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
